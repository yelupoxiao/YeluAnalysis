##  安装mysql

安装
```
yum install mysql
```
测试
```
[root@master badou2]# yum search mysql  # 客户端
[root@master badou2]# yum install mysql-server  # 服务端
```
启动
```
[root@master badou2]# /etc/init.d/mysqld starts  # 启动
```
查看启动状态
```
[root@master badou2]# netstat -anutp | grep mysql 
tcp        0      0 0.0.0.0:3306                0.0.0.0:*                   LISTEN      64196/mysqld        
[root@master badou2]
```
设置用户名和密码
```
[root@master badou2]# mysqladmin -u root password '111111'
```
测试登录是否成功 输入密码不显示
```
[root@master badou2]# mysql -u root -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3
Server version: 5.1.73 Source distribution

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>   # 出现此即为成功
```

## 安装hive


安装
```
[root@master badou2]# cd /usr/local/src

[root@master src]# tar -xvzf  apache-hive-0.13.0-bin.tar.gz 
[root@master src]# cd apache-hive-0.13.0-bin
[root@master apache-hive-0.13.0-bin]# cd conf/


  1 <configuration>
  2     <property>
  3         <name>javax.jdo.option.ConnectionURL</name>
  4         <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNoteExist=true</value>
  5     </property>
  6 
  7     <property>
  8         <name>javax.jdo.option.ConnectionDriverName</name>
  9         <value>com.mysql.jdbc.Driver</value>
 10     </property>
 11 
 12     <property>
 13          <name>javax.jdo.option.ConnectionUserName</name>
 14          <value>root</value>
 15      </property>
 16 
 17      <property>
 18          <name>javax.jdo.option.ConnectionPassword</name>
hive-site.xml                                                              1,1            Top
"hive-site.xml" 21L, 582C                                                 
[root@master conf]# 
```
修改bashrc
```
[root@master conf]# vim ~/.bashrc
 19 # hive conf
 20 # export HIVE_HOME=/usr/local/src/hive-0.12.0-bin
 21 export HIVE_HOME=/usr/local/src/apache-hive-0.13.0-bin
 ```
 添加$HIVE_HOME/bin:
 ```
 32 export PATH=$MAHOUT_HOME/bin:$ZOOKEEPER_HOME/bin:$HIVE_HOME/bin:HBASE_HOME/bin:$HBASE_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/bin:$PATH
 
 [root@master conf]# source ~/.bashrc  # 立即生效
```
安装驱动
```
 [root@master src]# pwd
/usr/local/src
[root@master src]# tar -xvzf mysql-connector-java-5.1.41.tar.gz 
[root@master src]# cd mysql-connector-java-5.1.41
```
把connector拷贝到hive的lib库中
```
[root@master mysql-connector-java-5.1.41]# cp mysql-connector-java-5.1.41-bin.jar /usr/local/src/apache-hive-0.13.0-bin/bin/lib

[root@master mysql-connector-java-5.1.41]# cd /usr/local/src/apache-hive-0.13.0-bin/lib 
```
先检查mysql启动状态
```
[root@master conf]# service mysqld status
mysqld (pid  64196) is running...
```
连接数据库
```
[root@master share_folder]# mysql -uroot -p111111
```
创建数据库
```
mysql> create database hive;  
```
启动hive
```
[root@master lib]# hive

Logging initialized using configuration in jar:file:/usr/local/src/apache-hive-0.13.0-bin/lib/hive-common-0.13.0.jar!/hive-log4j.properties
hive>     
# 若启动失败,可调用此命令定位bug
hive -hiveconf hive.root.logger=DEBUG,console
```
## Hive与传统RDBMS的区别

1、hive存储的数据量比较大，适合海量数据，适合存储轨迹类历史数据，适合用来做离线分析、数据挖掘运算，

事务性较差，实时性较差

 rdbms一般数据量相对来说不会太大，适合事务性计算，实时性较好，更加接近上层业务



2、hive的计算引擎是hadoop的mapreduce，存储是hadoop的hdfs文件系统，

 rdbms的引擎由数据库自己设计实现例如mysql的innoDB，存储用的是数据库服务器本地的文件系统



3、hive由于基于hadoop所以存储和计算的扩展能力都很好，

 rdbms在这方面比较弱，比如orcale的分表和扩容就很头疼



4、hive表格没有主键、没有索引、不支持对具体某一行的操作，适合对批量数据的操作，不支持对数据的update操作，

更新的话一般是先删除表然后重新落数据

 rdbms事务性强，有主键、索引，支持对具体某一行的增删改查等操作



5、hive的SQL为HQL，与标准的RDBMS的SQL存在有不少的区别，相对来说功能有限

rdbms的SQL为标准SQL，功能较为强大。



6、Hive在加载数据时候和rdbms关系数据库不同，hive在加载数据时候不会对数据进行检查，也不会更改被加载的数据文件，

而检查数据格式的操作是在查询操作时候执行，这种模式叫“读时模式”。在实际应用中，写时模式在加载数据时候会对列进行

索引，对数据进行压缩，因此加载数据的速度很慢，但是当数据加载好了，我们去查询数据的时候，速度很快。但是当我们的

数据是非结构化，存储模式也是未知时候，关系数据操作这种场景就麻烦多了，这时候hive就会发挥它的优势。

 rdbms里，表的加载模式是在数据加载时候强制确定的（表的加载模式是指数据库存储数据的文件格式），如果加载数据

时候发现加载的数据不符合模式，关系数据库则会拒绝加载数据，这个就叫“写时模式”，写时模式会在数据加载时候对数据模

式进行检查校验的操作。

最后做下总结，HIVE是数据仓库适合存储历史的海量的数据，适合做批量和海量复杂运算，事务性差，运算时间长。

RDBMS是数据库，存储数据量偏小一些，事务性强，适合做OLTP和OLAP业务，运算时间短。

参考资料:
https://blog.csdn.net/qq_42246689/article/details/84729795
https://blog.csdn.net/zx8167107/article/details/79114620

## HIve原理及架构图
参考资料:
https://blog.csdn.net/py_123456/article/details/80292267

## 查看表

```shell
# 查看表  show tables;
# 查看表 desc w_c
# 查询 select * from w_c;

# 查看hive 元数据库
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| hive               |
| hive_db            |
| mysql              |
| test               |
+--------------------+
5 rows in set (0.01 sec)

mysql> use hive;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;  # 元信息
+---------------------------+
| Tables_in_hive            |
+---------------------------+
| BUCKETING_COLS            |
| CDS                       |
| COLUMNS_V2                |
| DATABASE_PARAMS           |
| DBS                       |
| FUNCS                     |
| FUNC_RU                   |
| GLOBAL_PRIVS              |
| PARTITIONS                |
| PARTITION_KEYS            |
| PARTITION_KEY_VALS        |
| PARTITION_PARAMS          |
| PART_COL_STATS            |
| ROLES                     |
| SDS                       |
| SD_PARAMS                 |
| SEQUENCE_TABLE            |
| SERDES                    |
| SERDE_PARAMS              |
| SKEWED_COL_NAMES          |
| SKEWED_COL_VALUE_LOC_MAP  |
| SKEWED_STRING_LIST        |
| SKEWED_STRING_LIST_VALUES |
| SKEWED_VALUES             |
| SORT_COLS                 |
| TABLE_PARAMS              |
| TAB_COL_STATS             |
| TBLS                      |
| VERSION                   |
+---------------------------+
29 rows in set (0.00 sec)

mysql> desc VERSION;
+-----------------+--------------+------+-----+---------+-------+
| Field           | Type         | Null | Key | Default | Extra |
+-----------------+--------------+------+-----+---------+-------+
| VER_ID          | bigint(20)   | NO   | PRI | NULL    |       |
| SCHEMA_VERSION  | varchar(127) | NO   |     | NULL    |       |
| VERSION_COMMENT | varchar(255) | NO   |     | NULL    |       |
+-----------------+--------------+------+-----+---------+-------+
3 rows in set (0.00 sec)


mysql> select * from VERSION; 
+--------+----------------+------------------+
| VER_ID | SCHEMA_VERSION | VERSION_COMMENT  |
+--------+----------------+------------------+
|      1 | 0.13.0         | Set by MetaStore |
+--------+----------------+------------------+
1 row in set (0.00 sec)

mysql> desc TBLS;
+--------------------+--------------+------+-----+---------+-------+
| Field              | Type         | Null | Key | Default | Extra |
+--------------------+--------------+------+-----+---------+-------+
| TBL_ID             | bigint(20)   | NO   | PRI | NULL    |       |
| CREATE_TIME        | int(11)      | NO   |     | NULL    |       |
| DB_ID              | bigint(20)   | YES  | MUL | NULL    |       |
| LAST_ACCESS_TIME   | int(11)      | NO   |     | NULL    |       |
| OWNER              | varchar(767) | YES  |     | NULL    |       |
| RETENTION          | int(11)      | NO   |     | NULL    |       |
| SD_ID              | bigint(20)   | YES  | MUL | NULL    |       |
| TBL_NAME           | varchar(128) | YES  | MUL | NULL    |       |
| TBL_TYPE           | varchar(128) | YES  |     | NULL    |       |
| VIEW_EXPANDED_TEXT | mediumtext   | YES  |     | NULL    |       |
| VIEW_ORIGINAL_TEXT | mediumtext   | YES  |     | NULL    |       |
+--------------------+--------------+------+-----+---------+-------+
11 rows in set (0.00 sec)

# TBL_NAME 把hive 创建的表的名字维护到mysql里的TBL_NAME
mysql> select * from TBLS;
```



## Hive 创建,增删该查

```shell

# 删除  drop table 表名;

# hdfs 上
hadoop fs -ls /user/hive
hadoop fs -ls /user/hive/warehouse  # warejpise 存放hive 创建的表的名字

[root@master badou2]# cd hive_test/

[root@master hive_test]# vim create_ex_table.sql 

  create EXTERNAL TABLE w_a
   (
       usrid STRING,
       age STRING,
       sex STRING
   )
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
       LINES TERMINATED BY '\n';
   
  create EXTERNAL TABLE w_b
  (
      usrid STRING,
      active STRING,
      time STRING
  )
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
      LINES TERMINATED BY '\n'
                                                                                            
create_ex_table.sql                                                        1,1            All
"create_ex_table.sql" 17L, 316C

#  hive 中操作
# 创建外部表,删除后不影响数据,回复表结构后,数据还会存在
hive>   create EXTERNAL TABLE w_a
    >    (
    >        usrid STRING,
    >        age STRING,
    >        sex STRING
    >    )
    >    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
    >        LINES TERMINATED BY '\n';
OK
Time taken: 0.243 seconds
hive> show tables;                                     
OK
w_a

# 创建内部表,删除后hdfs上的数据也会删除
hive> create table w_aa as select * from w_a;

# sql脚本创建表
hive> drop table w_a;
[root@master hive_test]# hive -f create_ex_table.sql

# hive 创建了两个表
hive> show tables;
OK
w_a
w_aa
w_b
Time taken: 0.037 seconds, Fetched: 3 row(s)
hive> 

# 查看表结构内容
hive> desc w_a;
OK
usrid                   string                                      
age                     string                                      
sex                     string                                      
Time taken: 0.097 seconds, Fetched: 3 row(s)
hive> 

# 插入表内容  # 本地文件的导入方法load

hive> LOAD DATA LOCAL INPATH '/home/badou/badou2/hive_test/a.txt' OVERWRITE INTO TABLE w_a;
Copying data from file:/home/badou/badou2/hive_test/a.txt
Copying file: file:/home/badou/badou2/hive_test/a.txt
Loading data to table default.w_a
Moved to trash: hdfs://192.168.28.10:9000/user/hive/warehouse/w_a
Table default.w_a stats: [numFiles=1, numRows=0, totalSize=88, rawDataSize=0]
OK
Time taken: 0.264 seconds
hive> 
hive> select * from w_a;
OK
user1   27      1
user2   28      1
user3   29      0
user4   30      1
user5   31      0
user6   32      1
user7   33      1
user8   34      0
Time taken: 0.038 seconds, Fetched: 8 row(s)
hive> 

# hdfs 上也同步
[root@master hive_test]# hadoop fs -ls /user/hive/warehouse/w_a
Found 1 items
-rw-r--r--   3 root supergroup         88 2019-03-19 00:39 /user/hive/warehouse/w_a/a.txt
[root@master hive_test]# 


# w_b
hive> LOAD DATA LOCAL INPATH '/home/badou/badou2/hive_test/b.txt' OVERWRITE INTO TABLE w_b;
Copying data from file:/home/badou/badou2/hive_test/b.txt
Copying file: file:/home/badou/badou2/hive_test/b.txt
Loading data to table default.w_b
Moved to trash: hdfs://192.168.28.10:9000/user/hive/warehouse/w_b
Table default.w_b stats: [numFiles=1, numRows=0, totalSize=114, rawDataSize=0]
OK
Time taken: 0.3 seconds
hive> 

hive> select * from w_b;
OK
user1   100     20170301
user3   101     20170302
user4   102     20170303
user5   103     20170304
user7   104     20170305
user8   105     20170306
Time taken: 0.035 seconds, Fetched: 6 row(s)
hive> 


# 多表查询join 操作
hive> select A.userid,  A.age, B.time  from w_a A join w_b B on A.userid = B.userid;
FAILED: SemanticException [Error 10002]: Line 1:59 Invalid column reference 'userid'
hive> select A.usrid, A.age, B.time  from w_a A join w_b B on A.usrid = B.usrid;   
Total jobs = 1
Execution log at: /tmp/root/root_20190319005050_b2f098ef-3a10-478a-8521-bf78aee275af.log
2019-03-19 12:50:55     Starting to launch local task to process map join;      maximum memory = 1013645312
2019-03-19 12:50:55     Dump the side-table into file: file:/tmp/root/hive_2019-03-19_00-50-51_850_6060906636068614879-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2019-03-19 12:50:55     Uploaded 1 File to: file:/tmp/root/hive_2019-03-19_00-50-51_850_6060906636068614879-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile00--.hashtable (527 bytes)
2019-03-19 12:50:55     End of local task; Time Taken: 0.633 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201903060455_0022, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201903060455_0022
Kill Command = /usr/local/src/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201903060455_0022
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2019-03-19 00:51:00,799 Stage-3 map = 0%,  reduce = 0%
2019-03-19 00:51:03,819 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 0.7 sec
2019-03-19 00:51:04,829 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 0.7 sec
MapReduce Total cumulative CPU time: 700 msec
Ended Job = job_201903060455_0022
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.7 sec   HDFS Read: 323 HDFS Write: 108 SUCCESS
Total MapReduce CPU Time Spent: 700 msec
OK
user1   27      20170301
user3   29      20170302
user4   30      20170303
user5   31      20170304
user7   33      20170305
user8   34      20170306
Time taken: 13.019 seconds, Fetched: 6 row(s)
hive> 

# 实例二:
[root@master hive_test]# hadoop fs -put user_name.data.utf.txt /
# 创建表
 create EXTERNAL TABLE u_info
   (
       usrid STRING,
       age STRING,
       sex STRING
   )
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
       LINES TERMINATED BY '\n';
# 查看表内是否有数据,有的话则清除
hive> select * from u_info; 
OK
Time taken: 0.042 seconds
hive> 
# 清空表
hive> truncate table u_info;

# HDFS 上的文件与本地文件的区别就是一个有local,一个没有
hive> LOAD DATA INPATH '/user_name.data.utf.txt' OVERWRITE INTO TABLE u_info;  
Loading data to table default.u_info
Moved to trash: hdfs://192.168.28.10:9000/user/hive/warehouse/u_info
Table default.u_info stats: [numFiles=1, numRows=0, totalSize=532, rawDataSize=0]
OK
Time taken: 0.229 seconds
hive> 

hive> select * from u_info;
OK
704512  bj-华龙达       北京市华龙达商贸公司
2457600 nnzqd   南宁市百花谷文化传播有限公司
21889024        lyyzc@y.com     ztc
7979008 gx303yy200xlww014       中国人民解放军第三0三医院
7241856 科信环保设备    深圳市科信环保设备有限公司
1229056 杰瑞达工程      成都杰瑞达工程机械有限公司
10060032        tp-love 攀枝花市仁伟工贸有限公司
21692800        550166990@qq.com        ztc
3785088 佛罗米商贸      深圳市佛罗米商贸有限公司
21758464        山东蚂蚁搬家    山东蚂蚁搬家有限公司
Time taken: 0.044 seconds, Fetched: 10 row(s)
hive> 


# 查询所有时不启动mapreduce,查询一个时会启动mapreduce
hive> select * from u_info;
OK
704512  bj-华龙达       北京市华龙达商贸公司
2457600 nnzqd   南宁市百花谷文化传播有限公司
21889024        lyyzc@y.com     ztc
7979008 gx303yy200xlww014       中国人民解放军第三0三医院
7241856 科信环保设备    深圳市科信环保设备有限公司
1229056 杰瑞达工程      成都杰瑞达工程机械有限公司
10060032        tp-love 攀枝花市仁伟工贸有限公司
21692800        550166990@qq.com        ztc
3785088 佛罗米商贸      深圳市佛罗米商贸有限公司
21758464        山东蚂蚁搬家    山东蚂蚁搬家有限公司
Time taken: 0.044 seconds, Fetched: 10 row(s)
hive> desc u_info;
OK
usrid                   string                                      
age                     string                                      
sex                     string                                      
Time taken: 0.091 seconds, Fetched: 3 row(s)
hive>

hive> select age from u_info;
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201903060455_0023, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201903060455_0023
Kill Command = /usr/local/src/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201903060455_0023
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2019-03-19 01:27:51,322 Stage-1 map = 0%,  reduce = 0%
2019-03-19 01:27:53,328 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.62 sec
2019-03-19 01:27:56,369 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.62 sec
MapReduce Total cumulative CPU time: 620 msec
Ended Job = job_201903060455_0023
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.62 sec   HDFS Read: 761 HDFS Write: 144 SUCCESS
Total MapReduce CPU Time Spent: 620 msec
OK
bj-华龙达
nnzqd
lyyzc@y.com
gx303yy200xlww014
科信环保设备
杰瑞达工程
tp-love
550166990@qq.com
佛罗米商贸
山东蚂蚁搬家
Time taken: 9.757 seconds, Fetched: 10 row(s)
hive> 

# hdfs 上存在结果
[root@master hive_test]# hadoop fs -ls /user/hive/warehouse
Found 4 items
drwxr-xr-x   - root supergroup          0 2019-03-19 01:25 /user/hive/warehouse/u_info
drwxr-xr-x   - root supergroup          0 2019-03-19 00:39 /user/hive/warehouse/w_a
drwxr-xr-x   - root supergroup          0 2019-03-19 00:07 /user/hive/warehouse/w_aa
drwxr-xr-x   - root supergroup          0 2019-03-19 00:43 /user/hive/warehouse/w_b
[root@master hive_test]# 

# 导入
create xxx as select * from yy;
insert into table w_d select usrid,age from w_a limit 2;


hive>   create EXTERNAL TABLE w_d
       (
            usrid STRING,
             age STRING
            
        )
        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
           LINES TERMINATED BY '\n';
# 导入数据insert           
hive> insert into table w_d select usrid, age from w_a limit 2
    > ;
hive> select * from w_d;
OK
user1   27
user2   28
Time taken: 0.047 seconds, Fetched: 2 row(s)
hive> 

# 直接操作hdfs 移动命令到相应目录
[root@master hive_test]# hadoop fs -rmr /user/hive/warehouse/w_a/a.txt
Moved to trash: hdfs://192.168.28.10:9000/user/hive/warehouse/w_a/a.txt

[root@master hive_test]# hadoop fs -rmr /user/hive/warehouse/w_a
Moved to trash: hdfs://192.168.28.10:9000/user/hive/warehouse/w_a
[root@master hive_test]# hadoop fs -mkdir /user/hive/warehouse/w_a
[root@master hive_test]# 

[root@master hive_test]# hadoop fs -put a.txt   /user/hive/warehouse/w_a
[root@master hive_test]# hadoop fs -ls /
Found 14 items
-rw-r--r--   3 root supergroup     632207 2019-03-06 04:57 /The_Man_of_Property.txt
-rw-r--r--   3 root supergroup        404 2019-03-06 23:33 /b.txt
drwxr-xr-x   - root supergroup          0 2019-03-18 17:56 /hbase
-rw-r--r--   3 root supergroup       8711 2019-03-17 23:17 /input.data
-rw-r--r--   3 root supergroup   12224421 2019-03-07 03:44 /ip.lib.txt
drwxr-xr-x   - root supergroup          0 2019-03-17 23:53 /output
drwxr-xr-x   - root supergroup          0 2019-03-18 00:36 /output_hbase
drwxr-xr-x   - root supergroup          0 2019-03-07 03:45 /output_ip_lib
drwxr-xr-x   - root supergroup          0 2019-03-07 00:27 /output_sort
-rw-r--r--   3 root supergroup       3541 2019-03-07 03:45 /query_cookie_ip.txt.small
drwxr-xr-x   - root supergroup          0 2019-03-07 05:45 /test_dir
drwxr-xr-x   - root supergroup          0 2019-03-19 00:04 /tmp
drwxr-xr-x   - root supergroup          0 2019-03-19 00:03 /user
drwxr-xr-x   - root supergroup          0 2019-03-06 04:55 /usr
[root@master hive_test]# 

# 恢复后数据依然可读
hive> select * from w_a;
OK
user1   27      1
user2   28      1
user3   29      0
user4   30      1
user5   31      0
user6   32      1
user7   33      1
user8   34      0
Time taken: 0.056 seconds, Fetched: 8 row(s)
hive> 
```

**hive 导出**

```shell
hive> show tables;
OK
u_info
w_a
w_aa
w_b
w_d
Time taken: 0.024 seconds, Fetched: 5 row(s)
hive> 

hive> select * from w_a;
OK
user1   27      1
user2   28      1
user3   29      0
user4   30      1
user5   31      0
user6   32      1
user7   33      1
user8   34      0
Time taken: 0.402 seconds, Fetched: 8 row(s)
hive>  desc w_a;
OK
usrid                   string                                      
age                     string                                      
sex                     string                                      
Time taken: 0.119 seconds, Fetched: 3 row(s)
hive> 


[root@master hive_test]# pwd
/home/badou/badou2/hive_test
[root@master hive_test]# mkdir data
[root@master hive_test]# ls
1.txt  create_ex_table.sql   data          p1.txt         user_name.data.utf.txt
a.txt  create_partition.sql  hive_udf.jar  p2.txt
b.txt  create_table.sql      insert.sh     transform.awk
[root@master hive_test]# 

# hive 导出到本地localhost
# insert overwrite local directory '导出目录' sql 语句 (常用)
hive> insert overwrite local directory '/home/badou/badou2/hive_test/data/1.txt' select usrid,age from w_a;
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201903060455_0025, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201903060455_0025
Kill Command = /usr/local/src/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201903060455_0025
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2019-03-19 05:17:43,715 Stage-1 map = 0%,  reduce = 0%
2019-03-19 05:17:45,728 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.6 sec
2019-03-19 05:17:47,743 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.6 sec
MapReduce Total cumulative CPU time: 600 msec
Ended Job = job_201903060455_0025
Copying data to local directory /home/badou/badou2/hive_test/data/1.txt
Copying data to local directory /home/badou/badou2/hive_test/data/1.txt
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.6 sec   HDFS Read: 297 HDFS Write: 72 SUCCESS
Total MapReduce CPU Time Spent: 600 msec
OK
Time taken: 10.435 seconds
hive> 

[root@master 1.txt]# ls
000000_0

# 本地用sql语句
hive> select usrid,age from w_a;
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201903060455_0026, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201903060455_0026
Kill Command = /usr/local/src/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201903060455_0026
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2019-03-19 05:23:14,099 Stage-1 map = 0%,  reduce = 0%
2019-03-19 05:23:17,109 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.59 sec
2019-03-19 05:23:19,119 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.59 sec
MapReduce Total cumulative CPU time: 590 msec
Ended Job = job_201903060455_0026
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.59 sec   HDFS Read: 297 HDFS Write: 72 SUCCESS
Total MapReduce CPU Time Spent: 590 msec
OK
user1   27
user2   28
user3   29
user4   30
user5   31
user6   32
user7   33
user8   34
Time taken: 9.698 seconds, Fetched: 8 row(s)
hive> 

# 导出到hdfs上(常用)
hive> insert overwrite directory '/hive_data' select usrid, age from w_a; 
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201903060455_0027, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201903060455_0027
Kill Command = /usr/local/src/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201903060455_0027
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2019-03-19 05:28:40,915 Stage-1 map = 0%,  reduce = 0%
2019-03-19 05:28:42,922 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.57 sec
2019-03-19 05:28:44,940 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.57 sec
MapReduce Total cumulative CPU time: 570 msec
Ended Job = job_201903060455_0027
Stage-3 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
Stage-4 is filtered out by condition resolver.
Moving data to: hdfs://192.168.28.10:9000/tmp/hive-root/hive_2019-03-19_05-28-36_169_2725256477531312656-1/-ext-10000
Moving data to: /hive_data
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.57 sec   HDFS Read: 297 HDFS Write: 72 SUCCESS
Total MapReduce CPU Time Spent: 570 msec
OK
Time taken: 8.819 seconds
hive> 

[root@master 1.txt]# hadoop fs -text  /hive_data/000000_0
user127
user228
user329
user430
user531
user632
user733
user834
[root@master 1.txt]# 


# partion

[root@master hive_test]# hive -f create_partition.sql 

Logging initialized using configuration in jar:file:/usr/local/src/apache-hive-0.13.0-bin/lib/hive-common-0.13.0.jar!/hive-log4j.properties
OK
Time taken: 0.642 seconds
[root@master hive_test]# 


hive> desc p_t;
OK
usrid                   string                                      
age                     string                                      
dt                      string                                      
                 
# Partition Information          
# col_name              data_type               comment             
                 
dt                      string                                      
Time taken: 0.118 seconds, Fetched: 8 row(s)
hive> 


[root@master hive_test]# cat p1.txt 
user2   28      20170302
user4   30      20170302
user6   32      20170302
user8   34      20170302
[root@master hive_test]# 

hive> load data local inpath '/home/badou/badou2/hive_test/p1.txt' overwrite into table p_t partition(dt= '20170302');
Copying data from file:/home/badou/badou2/hive_test/p1.txt
Copying file: file:/home/badou/badou2/hive_test/p1.txt
Loading data to table default.p_t partition (dt=20170302)
Partition default.p_t{dt=20170302} stats: [numFiles=1, numRows=0, totalSize=72, rawDataSize=0]
OK
Time taken: 0.664 seconds
hive> 

hive> select * from p_t;
OK
user2   28      20170302
user4   30      20170302
user6   32      20170302
user8   34      20170302
Time taken: 0.058 seconds, Fetched: 4 row(s)
hive> 

# 换个partition 插入 (经常用,重要)
hive> load data local inpath '/home/badou/badou2/hive_test/p1.txt' overwrite into table p_t partition(dt= '20170303');
Copying data from file:/home/badou/badou2/hive_test/p1.txt
Copying file: file:/home/badou/badou2/hive_test/p1.txt
Loading data to table default.p_t partition (dt=20170303)
Partition default.p_t{dt=20170303} stats: [numFiles=1, numRows=0, totalSize=72, rawDataSize=0]
OK
Time taken: 0.397 seconds
hive> 

hive> select * from p_t;
OK
user2   28      20170302
user4   30      20170302
user6   32      20170302
user8   34      20170302
user2   28      20170303
user4   30      20170303
user6   32      20170303
user8   34      20170303
Time taken: 0.052 seconds, Fetched: 8 row(s)
hive> 

[root@master hive_test]# hadoop fs  -ls /user/hive/warehouse/
Found 6 items
drwxr-xr-x   - root supergroup          0 2019-03-19 05:44 /user/hive/warehouse/p_t
drwxr-xr-x   - root supergroup          0 2019-03-19 01:25 /user/hive/warehouse/u_info
drwxr-xr-x   - root supergroup          0 2019-03-19 02:22 /user/hive/warehouse/w_a
drwxr-xr-x   - root supergroup          0 2019-03-19 00:07 /user/hive/warehouse/w_aa
drwxr-xr-x   - root supergroup          0 2019-03-19 00:43 /user/hive/warehouse/w_b
drwxr-xr-x   - root supergroup          0 2019-03-19 02:12 /user/hive/warehouse/w_d
[root@master hive_test]# 

[root@master hive_test]# hadoop fs  -ls /user/hive/warehouse/p_t
Found 2 items
drwxr-xr-x   - root supergroup          0 2019-03-19 05:41 /user/hive/warehouse/p_t/dt=20170302
drwxr-xr-x   - root supergroup          0 2019-03-19 05:44 /user/hive/warehouse/p_t/dt=20170303
[root@master hive_test]# 

[root@master hive_test]# hadoop fs  -ls /user/hive/warehouse/p_t/dt=20170302
Found 1 items
-rw-r--r--   3 root supergroup         72 2019-03-19 05:41 /user/hive/warehouse/p_t/dt=20170302/p1.txt
[root@master hive_test]# 

hive> select * from p_t where dt= '20170302'; 按照往常会启动mapreduce 但这里不会
OK
user2   28      20170302
user4   30      20170302
user6   32      20170302
user8   34      20170302
Time taken: 0.138 seconds, Fetched: 4 row(s)
hive> 

# 不用partition
[root@master hive_test]# vim create_no_partition.sql 
  1 create TABLE p_t_2
  2 (
  3     usrid STRING,
  4     age STRING,
  5     dt  STRING
  6 )
  7 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
  8 LINES TERMINATED BY '\n';
  
  [root@master hive_test]# hive -f create_no_partition.sql 

Logging initialized using configuration in jar:file:/usr/local/src/apache-hive-0.13.0-bin/lib/hive-common-0.13.0.jar!/hive-log4j.properties
OK
Time taken: 0.777 seconds
[root@master hive_test]# 

Time taken: 0.075 seconds, Fetched: 3 row(s)
hive> select * from p_t_2;
OK
Time taken: 0.058 seconds
hive> load data local inpath '/home/badou/badou2/hive_test/p1.txt' overwrite into table p_t_2;

hive> select * from p_t_2;
OK
user2   28      20170302
user4   30      20170302
user6   32      20170302
user8   34      20170302
Time taken: 0.048 seconds, Fetched: 4 row(s)
hive> 

# 通过sql查询  无partition 会通过mapreduce 跑出来
hive> select * from p_t_2 where dt = '20170302';
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201903060455_0028, Tracking URL = http://master:50030/jobdetails.jsp?jobid=job_201903060455_0028
Kill Command = /usr/local/src/hadoop-1.2.1/libexec/../bin/hadoop job  -kill job_201903060455_0028
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2019-03-19 06:06:26,445 Stage-1 map = 0%,  reduce = 0%
2019-03-19 06:06:28,457 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.85 sec
2019-03-19 06:06:31,469 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.85 sec
MapReduce Total cumulative CPU time: 850 msec
Ended Job = job_201903060455_0028
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.85 sec   HDFS Read: 284 HDFS Write: 72 SUCCESS
Total MapReduce CPU Time Spent: 850 msec
OK
user2   28      20170302
user4   30      20170302
user6   32      20170302
user8   34      20170302
Time taken: 10.873 seconds, Fetched: 4 row(s)
hive> 



# udf 用户自定义函数
# 在sql 语句上些一个函数
# select fun(xxx) from table;

# 调用jar包
package com.badou.hive.udf:
import org.apache.hadoop.hive.ql.exec.UDF;
puplic class Uppercase extends UDF{
    public Text evaluate(final Text s){
        return new Text(s.toString().toUpperCase)
        }
}
# 打包成jar 上传到hive_test
# 导入jar 包
 hive > add jar /home/badou/hive_test/hive_udf.jar;
 hive > select * from p_t where dt = '20170302';
 hive > select usrid, age from p_t where dt = '20170302';
 hive > select usrid, Uppercase(usrid) age from p_t where dt = '20170302';
 
 ## 创建临时函数
 hive > create temporary Function uppercase as 'com.badou.hive.udf.Uppercase';
 hive > select usrid,uppercase(usrid), age from p_t where dt = '20170302';
 
 

```

## 拼接

```shell
[root@master hive_test]# cat transform.awk 
{
    print $1"_"$2
}
[root@master hive_test]# cat a.txt
user1   27      1
user2   28      1
user3   29      0
user4   30      1
user5   31      0
user6   32      1
user7   33      1
user8   34      0
[root@master hive_test]# cat a.txt | awk '{print $1"_"$2}'
use
```


## Hive内部表/外部表/分区
内部表：在Hive 中创建表时，默认情况下Hive 负责管理数据。即，Hive 把数据移入它的"仓库目录" (warehouse directory)

外部表：由用户来控制数据的创建和删除。外部数据的位置需要在创建表的时候指明。使用EXTERNAL关键字以后， Hìve 知道数据并不由自己管理，因此不会把数据移到自己的仓库目录。事实上，在定义时，它甚至不会检查这一外部位置是否存在。这是一个非常重要的特性，因为这意味着你可以把创建数据推迟到创建表之后才进行。

区别：丢弃内部表时，这个表(包括它的元数据和数据)会被一起删除。丢弃外部表时，Hive 不会碰数据，只会删除元数据，而不会删除数据文件本身。

Partition 分区表
    1).Partition 对应于数据库的Partition列的密集索引
    2).在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition的数据都存储在对应的目录中。
   

参考资料:
https://www.cnblogs.com/linn/p/6182624.html
