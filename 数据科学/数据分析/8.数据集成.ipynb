{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念释义\n",
    "数据集集成简单通俗地解释是把不同来源的数据合并存放在一起(如数据仓库)，从而方便后续数据挖掘工作。如果把数据挖掘比喻成炒菜，数据集成就是备菜的过程。\n",
    "在实际大数据项目中，80%的工作与数据集成相关。广义的数据集成包括数据清洗、数据抽取、数据集成和数据变换等操作。\n",
    "那么，数据集成有什么意义？在日常实际工作当中，我们需要的数据一般分布在不同的数据源中，而对这些数据进行规范化、标准化处理是必要的步骤，不然类似同一字段表达不同含义的情况会导致后续数据挖掘得到的结果失去意义。\n",
    "\n",
    "# 数据集成的两种架构\n",
    "数据工程师的主要工作内容包括数据ETL和数据挖掘的算法实现两部分。上面说了数据工程师大部分时间都在做ETL这部分工作。\n",
    "ETL英文是Extract、Transform、Load的缩写，即数据抽取、转换以及加载三个过程。\n",
    "数据抽取就是把数据从已有的数据源中提取出来。\n",
    "数据转换就是对原始数据进行处理，如把两种表合成一张。\n",
    "数据加载就是数据结果导出。\n",
    "根据转换发生的顺序和位置，数据集成可分为ETL和ELT两种架构。其中ETL目前是主流，而ELT是未来发展趋势。\n",
    "![1](./img/ETL和ELT流程示意图.jpg)\n",
    "# ELT工具\n",
    "典型的 ETL 工具有: \n",
    "商业软件：Informatica PowerCenter、IBM InfoSphereDataStage。Oracle Data Integrator、Microsoft SQL Server Integration Services 等\n",
    "开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等\n",
    "\n",
    "其中Kettle（“水壶”）是一个简单易用的工具，国内很多公司使用Kettle做数据集成。\n",
    "# Kettle的安装\n",
    "因为Kettle依赖Java运行环境（JRE），所以安装使用Kettle需要以下两个步骤：\n",
    "\n",
    "- [安装JDK]( http://www.oracle.com/technetwork/java/javase/downloads/index.html)\n",
    "- [安装Kettle](https://sourceforge.net/projects/pentaho/files/)\n",
    " ## 安装JDK\n",
    " 点击下载按钮进入下载界面 ，根据系统选择相应的版本下载。\n",
    "- 下载后安装JDK，安装过程中可以自定义安装目录等信息，例如我们选择安装目录为 C:\\Program Files\\Java\\jdk1.8.0_112。\n",
    "- 安装完成后，需要配置环境变量。\n",
    "1.右击\"我的电脑\"，点击\"属性\"，选择\"高级系统设置\"；\n",
    "2.选择\"高级\"选项卡，点击\"环境变量\"；\n",
    "3.在\"系统变量\"中设置3项属，JAVA_HOME,PATH,CLASSPATH 。若已存在则点击\"编辑\"，不存在则点击\"新建\"。\n",
    "```\n",
    "    变量设置参数如下：\n",
    "    变量名：JAVA_HOME\n",
    "    变量值：D:\\jdk       // 要根据自己的实际路径配置\n",
    "    变量名：CLASSPATH\n",
    "    变量值：.;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar;         //注意前面有个\".\"\n",
    "    变量名：Path\n",
    "    变量值：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin;\n",
    "```\n",
    "- 验证JDK安装是否成功。\n",
    "\"开始\"->\"运行\"，键入\"cmd\"，键入命令: java -version，出现如下版本信息，说明已安装成功。\n",
    "## 安装Kettle\n",
    " 打开安装kettle的链接你会发现进入的是一系列名为“Pentaho”的版本。Pentaho是一个以工作流为核心的、强调面向解决方案的开源商业智能（Business Intelligence, BI）套件，以构成全面的数据集成和业务分析平台。这些套件各自为独立产品，之间为松耦合可插拔式设计，用户可根据自身需求进行灵活选择。Kettle是Pentaho整个产品体系中的数据集成模块，使用突破性的元数据驱动方法提供强大的“提取，转换和加载（ETL）”功能。粗略类比下，Pentaho是腾讯全家桶（如QQ音乐、动漫、空间、邮箱、视频等），Kettle就是QQ本尊。更准确的说法是Pentaho是套件，Kettle是其组件，若想了解具体详情可自行搜索两者的区别。\n",
    " 另外需要注意的是，JDK版本不能太老，否则会启动失败。如果JDK是1.6版本的，Data Integration 6和7就无法启动，那就用Data Integration 5 。\n",
    " 我下载了7.0版本 pdi-ce-7.0.0.0-25.zip，解压后直接双击Spoon.bat运行。启动需要等一小会儿，然后进入kettle的主界面。大功告成！\n",
    " \n",
    " # Kettle的使用\n",
    " Kettle采用可视化方式来对数据库间的数据进行操作迁移，包括了Transformation转换和Job作业。\n",
    " 转换对数据操作进行了定义，而数据操作就是数据从输入到输出的一个过程。作业负责将转换组织起来完成某项作业。在通常的工作中，我们会把任务分解成不同的作业，然后再把作业分解成多个转换。\n",
    " 转换可以为分成三个步骤，包括输入、中间转换以及输出。其中有两个主要概念：Step和Hop。前者是步骤，后者是跳跃线。每个步骤完成一个特定功能，然后通过跳跃线的连接表示数据的流向。\n",
    " 至于作业如何理解，其实一个完成的任务，就是讲创建好的转换和作业串联起来。作业中也有两个主要概念，即Job Entry（工作实体）和Hop。每个工作实体都用来执行具体的任务，比如调用转换、发送邮件等。通过Hop将工作实体连接起来，并制定相应的执行条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下提供联系数据集可尝试操作下：\n",
    "[测试数据集](./data/数据集成-数据集.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kettle安装MySQL连接驱动\n",
    "当第一次使用Kettle连接MySQL可能会报错,这时需要到[MySQL官网下载](https://dev.mysql.com/downloads/connector/j/)相应驱动进行安装，这里已经给出了链接地址，你可以直接点击蓝字安装。\n",
    "![1](./img/连接MySQL驱动.png)\n",
    " 记得解压之后将文件mysql-connector-java-8.0.16.jar复制到Pentaho Data Integration所在目录的lib文件夹下，重启Spoon，重新配置并测试即可 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis]",
   "language": "python",
   "name": "conda-env-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
