{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在现实生活中，我们会遇到各种选择，不论是选择男女朋友，还是挑选水果，都是基于遗忘的经验来做判断。如果把判断背后的逻辑整理成一个结构图，你会发现它实际上是一个树状图，即决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/决策树之打篮球.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树算法是经常使用的数据挖掘算法,这是因为决策树就像一个人脑的决策模型一样,呈现出来非常直观.决策树分类广泛应用于各行各业,比如金融行业做货代风险评估,医疗行业用决策树生成辅助诊断,电商行业用于预测销售额等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原理概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造与剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在做决策树的时候会经历两个阶段:第一个阶段是构造,即生成一个完成的决策树;第二个阶段是剪枝,即给决策树瘦身,以防止过拟合."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造的过程就是选择什么属性作为节点的过程.在决策树中存在三种节点:\n",
    "\n",
    "- 根节点:就是树的最顶端最开始的那个节点,如上图的天气就是一个根节点;\n",
    "- 子节点:就是中间的那些节点,如上图的温度.湿度,刮风;\n",
    "- 叶节点:就是树最底部的节点,即决策结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "节点之间存在父子关系.在构造过程中,那你要解决三个主要问题:\n",
    "\n",
    "- 选择哪个属性作为根节点;\n",
    "- 选择哪些属性作为子节点;\n",
    "- 什么时候停止并得到目标状态,即叶节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪枝的目的是为了不需要做太多判断就可以的到不错的结果,防止过拟合现象发生.过拟合简单理解就是模型的训练结果太好了,以至于变得很死板,换到别的地方在实际应用中就会导致分类错误.\n",
    "\n",
    "导致过拟合的原因之一就是因为训练集中样本量较小,将局部当做整体导致真实数据分类中出现错误,即模型返化能力差,不适合解决一般问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对决策树进行剪枝一般有两种做法,一是预剪枝,二是后剪枝,前者是在构造时就进行剪枝,后者是在生成决策树之后再进行剪枝.预剪枝减掉的节点都是对那些划分没有意义、在验证集中不能带来准确性提升的节点,这时会将当前节点当做叶节点,不再对其进行划分.后剪枝通常会从叶节点开始逐层向上,如果减掉一个节点子树对分类准确性影响不大或者提升了分类的准确性,那就用这个节点子树的叶节点来替代该节点,类则标记为这个节点子树最频繁的那个类."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 纯度与信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的构造过程相当于寻找纯净划分的过程.在分类中纯度越高,分歧越小.比如有3个集合,集合1是6次都去打篮球,集合2是4次去打篮球2次不去打篮球,集合3是3次去打篮球3次不去打篮球.按照纯度指标来说,集合1>集合2>集合3,因为集合1分歧最小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵是指信息的不确定度.在信息论中,随机离散事件出现的概率存在不确定性,引入信息熵的概念正是为了度量这种信息的不确定性.**信息熵的数学表达式:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Entropy(t) = - \\sum_{i=0}^{c-1}p(i|t)log_2p(i|t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式中p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2为取以2为底的对数.当不确定性越大时,它所包含的信息量也就越大,信息熵也就越高."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例说明，假设有2个集合：\n",
    "\n",
    "- 集合1:5次去打篮球，1次不去打篮球；\n",
    "- 集合2:3次去打篮球，3次不去打篮球。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在集合1中，有6次决策，其中打篮球是5次，不打篮球是1次。那么假设类别1为打篮球，即次数为5；类别2为不打篮球，即次数为1.这样的话节点划分为类别1的概率是5/6,为类别2的概率是1/6,带入上述信息熵公式得:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Entropy(t)= - (1/6)log_2(1/6) - (5/6)log_2(5/6) = 0.65\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样集合2中,类别1中打篮球的次数是3,类别2中不打篮球的次数也是3,计算信息熵得:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Entropy(t) = - (3/6)log_2(3/6) - (3/6)log_2(3/6) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的计算结果可以看到,信息熵越大,纯度越低.当集合中的所有样本均匀混合时,信息熵最大,纯度最低."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们会基于纯度来构建决策树.经典的\"不纯度\"指标有三种,分别是信息增益(ID3算法),信息增益率(C4.5算法)以及基尼系数(Cart算法).三种算法基于信息度量的不同方式.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓信息增益是指通过划分可以提高纯度,降低信息熵.计算公式是父节点的信息熵减去所有子节点的信息熵.在计算过程中,通过计算每个子节点的归一化信息熵,即按照每个子节点在父节点中出现的概率,来计算这些子节点的信息熵.**信息增益表达公式:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Gain(D,a) = Entropy(D) - \\sum_{i=1}^k\\frac{|D_i|}{|D|}Entropy(D_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式中D代表父节点,Di代表子节点,Gain(D,a)中的a作为D节点的属性选择."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设天气=晴的时候,会有5次去打篮球,5次不去打篮球..其中D1刮风=是,有2次打篮球,1次不打篮球;D2刮风=否,有3次打篮球,4次不打篮球.那么a代表节点的属性,即天气=晴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/决策树之信息增益.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么,D作为节点的信息增益可表达为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Gain(D,a) = Entropy(D)- \\left(\\frac{3}{10}Entropy(D_i)+\\frac{7}{10}Entropy(D_2)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里注意两个子节点归一化信息熵分别为3/10的D1信息熵和7/10的D2信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里准备了打篮球的数据集,训练数据如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/决策树之数据集.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过观察发现,训练集中一共有7条数据,3个打篮球,4个不打篮球,所以根节点的信息熵是:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Entropy(D)= - \\sum_{k=1}^2p_klog_2p_k = - (\\frac{3}{7}log_2\\frac{3}{7}+\\frac{4}{7}log_2\\frac{4}{7}) = 0.985\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果将天气作为属性划分会有三个子节点,分别是D1晴天,D2阴天,D3小雨.用 + 表示去打篮球,用 - 表示不去打篮球.那么采取如下记录方式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D1(天气=晴天)=(1-,2-,6+)  \n",
    "D2(天气=阴天)=(3+,7-)  \n",
    "D3(天气=小雨)=(4+,5-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别计算三个子节点的信息熵:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Ent(D_1)= - \\left(\\frac{1}{3}log_2\\frac{1}{3} + \\frac{2}{3}log_2\\frac{2}{3}\\right) = 0.918\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Ent(D_2)= - \\left(\\frac{1}{2}log_2\\frac{1}{2} + \\frac{1}{2}log_2\\frac{1}{2}\\right) = 1.0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Ent(D_3)= - \\left(\\frac{1}{2}log_2\\frac{1}{2} + \\frac{1}{2}log_2\\frac{1}{2}\\right) = 1.0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中D1有3条记录,D2有2条记录,D3有2条记录,所以D中一共有7条记录.那么D1在D中的概率是3/7,D2是2/7,D3是2/7.作为子节点的归一化信息熵 = 3/7 * 0.918 + 2/7 * 1.0 + 2/7 * 1.0 = 0.965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在ID3中通过信息增益来构造决策树,所以要计算每个节点的信息增益.天气作为属性节点的信息增益为,Gain(D,天气)=0.985 - 0.965 = 0.020.其他属性作为根节点的信息增益分别为:\n",
    "\n",
    "Gain(D,温度)=0.128  \n",
    "Gain(D,湿度)=0.020  \n",
    "Gain(D,刮风)=0.020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到温度作为属性的信息增益最大.在ID3算法中将信息增益最大的节点作为父节点,这样就可以得到纯度高的决策树.所以讲温度作为根节点,其决策树状图分裂如下图所示:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/决策树之树状图分裂.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上图第一个子节点,也就是D1={1-,2-,3+,4+}进一步分裂,往下划分,计算其不同属性(天气,湿度,刮风)作为节点的信息增益,得到:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gain(D , 湿度)=1  \n",
    "Gain(D , 天气)=1  \n",
    "Gain(D , 刮风)=0.3115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到湿度或者天气作为D1的节点可以得到最大信息增益,任意选择其中一个如湿度作为节点的属性划分.同理,按照上面步骤得到完整的决策树:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/决策树之完整决策树.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3的优势是算法规则简单,可解释性强,但也存在缺陷,比如ID3算法倾向于选择取值比较多的属性.假如将ID编号作为属性,那么ID编号将会被选为最优属性,可实际上编号作为属性没有意义,对打篮球的分类并没有作用."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对于ID3算法,C4.5算法进行了如下几方面的改进:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采集信息增益率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益率=信息增益/属性熵 为了避免ID3算法中倾向于选取值多属性的问题,C4,5选择信息增益率的方式来选择属性.\n",
    "\n",
    "当属性很多值时,相当于被划分为很多分,虽然信息增益变大,但属性熵也会变大,因此整体的信息增益率并不大."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采用悲观剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "悲观剪枝(PEP)是后剪枝技术的一种,通过递归估算每个内部节点的分类错误率,比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝.这种剪枝方法不再需要一个单独的测试数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离散化处理连续属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C4.5可以处理连续属性,如湿度属性不按照高中低划分,而按照湿度值计算.C4.5选择具有最高信息增益的划分所对应的的阈值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对数据集不完整的情况,在不考虑缺失数值的情况下,C4,5算法仍然能够通过计算信息增益对属性进行划分选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID3算法的优点是简单易懂,缺点是对噪声敏感,训练集如果有少量错误,可能会导致分类错误.C4,5在ID3基础上,用信息增益率代替信息增益,解决了噪声敏感问题,并且可以对构造树进行剪枝,处理连续数值以及缺失值等情况,但由于C4,5需要对数据集进行多次扫描,导致算法效率相对较低.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART算法(Classification And Regression Tree),即分类回归树.ID3和C4.5算法可以生成二叉树或多叉树,而CART只支持二叉树,同时CART决策树既可以作为分类树,也可以作为回归树."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类树与回归树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分组归类用分类树,预测估算用回归树.比如给出一个数据集,包括靴子时间,性别,职业,年龄四个字段,基于这些数据想要判断一个人的职业身份,这个属于分类树,因为是要从几个分类中做选择.如果是给定了数据,想要预测一个人的年轻,那就属于回归树."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类树可以处理离散数据,输出的是样本的类别.回归树可以对连续性数值进行预测,输出的是一个数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基尼系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面讲到过基于信息度量的不同方式,决策树可分为ID3,C4.5以及CART三种算法.其实,CART与C4.5算法类似,只是属性选择的指标采用的是基尼系数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基尼系数常常被用来衡量一个国家收入差距.当基尼系数大于0.4时,说明财务差异悬殊;当基尼系数在0.2-0.4之间说明分配合理,财富差距不大.基尼系数本身反映了样本的不确定度,基尼系数越小说明样本之间差异性越小,不确定程度低.而分类的过程本身是一个不确定度降低的过程,即纯度提升的过程.所以**CART算法在构造分类树时会选择基尼系数最小的属性作为属性的划分.** 基尼系数的计算公式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "GINI(t) = 1 - \\sum_k [p(C_k | t) ]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t表示节点,p(Ck|t)表示节点t属于类别Ck的概率,节点t的基尼系数为1减去各类别Ck概率的平方和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**举例说明:  \n",
    "集合1: 6个都去打篮球;  \n",
    "集合2: 3个去打篮球,3个不去打篮球**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对集合1,所有人都去打篮球,所以p(Ck|t)=1,那么GINI(t)=1-1=0;  \n",
    "针对集合2,有一半人去打篮球,另一半不去打篮球,所以p(C1|t)=0.5,p(C2|t)=0.5,GINI(t)=1-(0.5* 0.5+0.5 * 0.5)= 0.5  \n",
    "比较以上结果,集合1的基尼系数最小,证明样本最稳定,而集合2的样本不稳定性更大."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 工作流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CART算法中基于基尼系数对特征属性进行二元分类.假设属性A将节点D划分为D1和D2,那么如下图所示:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/决策树之CART算法二元分类.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "节点D的基尼系数等于D1和D2的归一化基尼系数之和,用公式表示为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "GINI(D,A)=\\frac{D_1}{D}GINI(D_1) + \\frac{D_2}{D}GINI(D_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化基尼系数代表每个子节点基尼系数乘以该节点占整体父节点D中的比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们已经计算出D1和D2的基尼系数,分别为0和0.5.所以按照公式在属性A的划分下,节点D的基尼系数为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "GINI(D,A)=\\frac{6}{12}GINI(D_1) +\\frac{6}{12}GINI(D_2) =0.25\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "节点D被属性A划分后的基尼系数越大,样本集合的不确定性越大,也就是不纯度越高."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何创建分类树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以利用sklearn库中的DecisionTreeClassifier类直接创建分类树,其中criterion这个参数默认等于gini,即默认采用CART分类树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART 分类树准确率 0.9600\n"
     ]
    }
   ],
   "source": [
    "# 获取数据集\n",
    "iris = load_iris()\n",
    "# 获取特征集和分类标识\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "# 随记抽取33%的数据作为测试集,其余为训练集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=0.33, random_state=0)\n",
    "# 创建CART分类树\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "# 拟合构造CART分类树\n",
    "clf = clf.fit(train_features, train_labels)\n",
    "# 用CART分类树做预测\n",
    "test_predict = clf.predict(test_features)\n",
    "score = accuracy_score(test_labels, test_predict)\n",
    "print(\"CART 分类树准确率 {:.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何创建回归树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与分类树划分数据集的过程类似，不过回归树得到的预测结果是连续值，而且评价\"不纯度\"的指标也变成了通过评价样本的离散程度来评价\"不纯度\".  \n",
    "假设x为样本的个体,均值为μ.统计样本的离散程度可以通过取差值的绝对值或方差."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "差值的绝对值为样本值减去样本均值的绝对值:\n",
    "$$\n",
    "|x - μ |\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差为每个样本值减去样本均值的平方和除以样本个数:\n",
    "$$\n",
    "s = \\frac{1}{n}\\sum(x - μ)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上两种节点划分标准分别对应着两种目标函数最优化标准,即最小绝对偏差(LAD)和最小二乘偏差(LSD).通常使用最小二乘偏差的情况较多."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用sklearn自带波士顿房价数据集进行cart回归树预测\n",
    "# encoding=utf-8\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "回归树二乘偏差均值: 17.0422754491\n",
      "回归树绝对值偏差均值: 2.92934131737\n"
     ]
    }
   ],
   "source": [
    "# 准备数据集\n",
    "boston = load_boston()\n",
    "# 探索数据\n",
    "print(boston.feature_names)\n",
    "# 获取特征集和房价\n",
    "features = boston.data\n",
    "prices = boston.target\n",
    "# 随机抽取33%的数据作为测试集,其余为训练集\n",
    "train_features, test_features, train_price, test_price = train_test_split(\n",
    "    features, prices, test_size=0.33)\n",
    "# 创建CART回归树\n",
    "dtr = DecisionTreeRegressor()\n",
    "# 拟合构造CART回归树\n",
    "dtr.fit(train_features, train_price)\n",
    "# 预测测试集中的房价\n",
    "predict_price = dtr.predict(test_features)\n",
    "# 测试集的结果评价\n",
    "print(\"回归树二乘偏差均值:\", mean_squared_error(test_price, predict_price))\n",
    "print(\"回归树绝对值偏差均值:\", mean_absolute_error(test_price, predict_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART决策树的剪枝采取代价复杂度(CCP)方法,其是后剪枝的一种,全称cost-complexity prune.这种剪枝方式用到一个叫做**节点的表面误差率增益值**的指标,其公式如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "α=\\frac{C(t)-C(T_t)}{|T_t|-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tt表示代表以t为根节点的子树,C(Tt)表示节点t的子树没被剪枝时子树Tt的误差,C(t)表示节点t的子树被剪枝后节点t的误差,|Tt|代表子树Tt的叶子数,剪枝后T的叶子数减少了|Tt|-1.所以节点的表面误差率增益值等于节点t的子树被剪枝后的误差变化除以减掉的叶子数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要寻找的是最小α值对应的节点,把它减掉,这时候剪枝前后误差最小.重复以上过程直至只剩下根节点,即最后一个子树.得到剪枝后的子树集合后,我们需要用验证集对所有子树的误差计算一遍.可以通过计算每个子树的基尼指数或者平方误差,取误差最小的那棵,从而得到我们想要的结果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**综上所述,  \n",
    "ID3算法,基于信息增益做判断;  \n",
    "C4.5算法,基于信息增益率做判断;  \n",
    "CART算法,分类树是基于基尼系数做判断.回归树是基于偏差做判断.   \n",
    "这三个指标也是计算\"不纯度\"的三种计算方式**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn 中自带的决策树分类器 DecisionTreeClassifier,相关参数如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier(\n",
    "    class_weight=None,\n",
    "    criterion='entropy',\n",
    "    max_depth=None,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    presort=False,\n",
    "    random_state=None,\n",
    "    splitter='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了设置criterion采用不同的决策树算法外,如entropy表示基于信息熵,即ID算法,其结果与C4,5相差不大，如gini为默认参数，基于基尼系数，creterion=gini时执行的是CART算法。至于其他参数建议建议使用默认.默认参数不会限制决策树的最大深度,不限制叶子点数,认为所有分类的权重都相等."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/sklearn决策树参数解释.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造完决策树分类器后,可以使用fit方法让分类器进行拟合,然后使用predict方法对新数据进行预测,得到预测的分类结果可以通过score方法得到分类器的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|方法表|作用|\n",
    "|---|---|\n",
    "|fit(features,labels)|通过特征矩阵,分类标识,让分类器进行拟合|\n",
    "|predict(features)|返回预测结果|\n",
    "|score(features,labels)|返回准确率|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "泰坦尼克号是著名的十大灾难之一,究竟多少人遇难,各方统计的结果不一.这是一个生存预测问题.我们需要用哪个决策树分类对训练集进行训练,针对测试集中的乘客进行生存预测,并告知分类器的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练集中,包括以下字段:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字段|描述\n",
    "---|---\n",
    "Passengerld|乘客编号\n",
    "Survived|是否生存\n",
    "Pclass|船票等级\n",
    "Name||乘客姓名\n",
    "Sex|乘客性别\n",
    "SibSp|亲戚数量(兄妹,配偶数)\n",
    "Parch|亲戚数量(父母,子女数)\n",
    "Ticket|船票号码\n",
    "Fare|船票价格\n",
    "Cabin|船舱\n",
    "Embarked|登陆港口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练集中乘客的生存进行预测,可划分为两个重要阶段,即准备阶段和分类阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/项目之生存预测关键流程.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备阶段:** 通过对训练数据集和测试数据集进行探索以便分析数据质量,在经过数据清洗后,通过特征选择实现对数据的降维,便于后续分类运算  \n",
    "**分类阶段:** 通过训练集的特征矩阵,分类结果得到决策树分类器,然后将分类器应用于测试集.最后我们对决策树分类器的准确性进行分析并对决策树模型进行可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据探索对分类器没有实质作用但不可忽略,因为只有足够了解这些数据的特性,才能帮助我们进行后续的数据清洗与特征选择."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用于数据探索的函数如下:\n",
    "\n",
    "- 使用info()了解数据表的基本情况:行数,列数,每列的数据类型以及数据完整度\n",
    "- 使用describe()了解数据表的统计情况:总数,平均值,标准差,最小值,最大值等\n",
    "- 使用describe(include=['O'])查看字符串类型(非数字)的整体情况\n",
    "- 使用head查看前几行数据(默认为前5行)\n",
    "- 使用tail查看后几行数据(默认为后5行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "train_data = pd.read_csv('./data/Titanic_Data/train.csv')\n",
    "test_data = pd.read_csv('./data/Titanic_Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "None\n",
      "------------------------------\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "------------------------------\n",
      "                    Name   Sex Ticket    Cabin Embarked\n",
      "count                891   891    891      204      889\n",
      "unique               891     2    681      147        3\n",
      "top     Fry, Mr. Richard  male   1601  B96 B98        S\n",
      "freq                   1   577      7        4      644\n",
      "------------------------------\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "------------------------------\n",
      "     PassengerId  Survived  Pclass                                      Name  \\\n",
      "886          887         0       2                     Montvila, Rev. Juozas   \n",
      "887          888         1       1              Graham, Miss. Margaret Edith   \n",
      "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
      "889          890         1       1                     Behr, Mr. Karl Howell   \n",
      "890          891         0       3                       Dooley, Mr. Patrick   \n",
      "\n",
      "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
      "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
      "887  female  19.0      0      0      112053  30.00   B42        S  \n",
      "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
      "889    male  26.0      0      0      111369  30.00  C148        C  \n",
      "890    male  32.0      0      0      370376   7.75   NaN        Q  \n"
     ]
    }
   ],
   "source": [
    "# 数据探索\n",
    "print(train_data.info())\n",
    "print('-' * 30)\n",
    "print(train_data.describe())\n",
    "print('-' * 30)\n",
    "print(train_data.describe(include=['O']))\n",
    "print('-' * 30)\n",
    "print(train_data.head())\n",
    "print('-' * 30)\n",
    "print(train_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据探索,发现age,cabin和embarked数据有缺失.其中age为年龄字段,是数值型,我们可以通过平均值进行补齐;fare为船票价格,是数值型,处理方式同age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用平均年龄来填充年龄中的nan值\n",
    "train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean, inplace=True)\n",
    "# 使用票价均值填充票价中的nan值\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin为船舱,有大量缺失值,在训练集和测试集中的缺失率分别为77%和78%,无法补齐;Embarked为登陆港口,为少量缺失值,我们可以把缺失值补齐.\n",
    "\n",
    "首先观察Embarked字段的取值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['Embarked'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现一共3个登陆港口,其中S最多,因此我们将其余缺失Embarked的数值均设置为S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Embarked'].fillna('S', inplace=True)\n",
    "test_data['Embarked'].fillna('S', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**特征选择是分类器的关键,特征选择不同,得到的分类器也不同.**\n",
    "\n",
    "通过数据探索发现,Passengerld为乘客编号对分类没有作用,Name为乘客姓名对分类无用,Cabin字段缺失值太多,Ticket为船票号码杂乱无规律,以上字段均可以放弃.剩余字段包括Pclass、Sex、Age、SibSp、Parch 和 Fare ,这些属性和乘客的生存预测分类有关系.至于具体什么关系,我们可以交给 分类器来处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们将剩余字段作为特征,放到特征向量features里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征选择\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**特征值中有一些是字符串不方便后续运算,需转成数值类型.**  \n",
    "比如Sex字段,有male和female两种取值,可以变成Sex=male和Sex=female连个字段,数值用0或1来表示 .  \n",
    "Embarkde有S,C,Q三种可能,可以改成Embarked=S,Embarked=C和Embarked=Q三个字段,数值用0或1来表示."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用sklearn特征选择中的**DictVectorizer类**,用它将可以处理符号化的对象,将符号转成数字0/1进行表示,具体方法如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvec = DictVectorizer(sparse=False)\n",
    "train_features = dvec.fit_transform(train_features.to_dict(orient='record'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform函数将特征向量转化为特征矩阵.我们看下dvec在转化后的特征属性是怎样的,即查看dvec的feature_names_属性值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']\n"
     ]
    }
   ],
   "source": [
    "print(dvec.feature_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样train_features特征矩阵就包括了10个特征值（列）以及891个样本（行）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里选择ID3算法,即创建DecisionTreeClassifier时设置criterion='entropy'\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 准确率为 0.9820\n"
     ]
    }
   ],
   "source": [
    "# 构造ID3决策树\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "# 决策树训练\n",
    "clf.fit(train_features, train_labels)\n",
    "# 得到决策树准确率\n",
    "acc_decision_tree=round(clf.score(train_features,train_labels),6)\n",
    "print('score 准确率为 {:.4f}'.format(acc_decision_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们没有测试集的实际结果,因此无法用测试集的预测结果与实际结果做对比.这里用训练集进行训练,再用训练集自身做准确率评估结果自然会很高,就好比\"王婆卖瓜自卖自夸\".很明显这样做无法对分类器在实际环境下做准确率的评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测&评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对以上问题,我们通常采用K折交叉验证的方式来统计决策树分类器的准确率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证是一种常用的验证分类准确率的方法,原理是拿出大部分样本进行训练,少量的用于分类器的验证.K折交叉验证,就是做K次交叉验证,每次选择K分之一的数据作为验证,其余的作为训练.轮流K次,最后去平均值.K折交叉验证的过程可梳理如下:  \n",
    "- 将数据集平均分割成K个等份;\n",
    "- 使用1份数据作为测试数据,其余作为训练数据;\n",
    "- 计算测试准确率\n",
    "- 使用不同的测试集,重复2,3步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score 准确率为0.7735\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 使用K折交叉验证统计决策树准确率\n",
    "print(u'cross_val_score 准确率为{:.4f}'.format(np.mean(cross_val_score(clf,train_features,train_labels,cv=10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn的model_selection模型选择中提供了cross_val_score函数,该函数中的参数cv代表对原始数据划分成多少分,也就是K值,一般建议K取值10.  \n",
    "显然,从结果来看,对于不知道测试集实际结果的数据集来说,要使用K折交叉验证才能知道模型的准确率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用Graphviz可视化工具帮助我们把决策树呈现出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装Graphviz库步骤:  \n",
    "- 安装Graphviz工具[下载地址](http://www.graphviz.org/download/);\n",
    "- 将Graphviz添加到环境变量PATH中[参考教程](https://blog.csdn.net/lanchunhui/article/details/49472949)\n",
    "- 需要Graphviz库,没有的话可以使用如pip install graphviz安装即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键点梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特征选择是分类模型好坏的关键.选择什么样的特征以及对应的特征值矩阵,决定了分类模型的好坏.通常情况下,特征值不都是数值类型,可以使用DictVectorizer类进行转化;\n",
    "- 模型准确率需要考虑是否有测试集的实际结果做对比,当测试集没有真实结果可做对比时,需要使用K折交叉验证cross_val_score;\n",
    "- Graphviz可视化工具可以方便地将决策模型呈现出来,帮助你更好地理解决策树的构建."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis]",
   "language": "python",
   "name": "conda-env-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
