{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯分类最适合的场景就是文本分类,情感分析以及垃圾邮件识别.可以看到这三个场景本质上都是文本分类.所以朴素贝叶斯常用于自然语言处理NLP的工具."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原理概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你不能准确预知一个事物本质的时候,你可以依靠和事物本质相关的事件来进行判断,如果事情发生的频次多,则证明这个属性更有可能存在.比如一个经常各种花钱的人可能是个有钱人.虽然你不能明确地知道他是否有钱,但你通过他总是花钱这个事件做出了他很钱的判断.贝叶斯原理就是对生活中这类事情进行解释的数学模型并为预测推断提供了可量化的数学公式."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逆向概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆向概率是相对正向概率而言.正向概率很容易理解,比如袋子里有N个球,不是白球就是黑球,其中M个黑球,那么从中摸出一个球是黑球的概率我们是可以推算出来的.像这种了解事情的全貌再做判断,被称之为上帝视角.显而易见是在现实生活中是难以实现的.  \n",
    "我们遇到的现实问题,更多的是如下情况:我们事先不知道袋子里黑白球的比例,能不能通过我们摸出来的球的颜色,反过来去推断袋子里面黑白球的比例呢?  \n",
    "我们知道这是可以实现的.原理就是贝叶斯原理所阐释的.贝叶斯原理与其他统计学推断方法截然不同,它是建立在主观判断的基础上:在我们不了解所有客观事实的情况下,同样可以先估计一个值,然后根据实际结果不断进行修正."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过经验来判断事情发生的概率.比如南方梅雨季就是通过往年的气候总结出来的经验,这个时候下雨的概率就比其他时间高出很多.再比如每年双11过后全国快递单量会出现猛增"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是发生结果后,推测原因的概率.比如上周A城市交通事故异常频发,经过调研得出原因可能是甲乙丙,那么A城市上周交通事故异常频发是因为原因甲的概率就是后验概率.后验概率属于条件概率的一种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事件A在另外一个事件B已经发生条件下的发生概率,表示P(A|B),读作\"在B发生的条件下A发生的概率\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以把概率模型的训练过程理解为求参数估计的过程.比如一个硬币在10次抛落中正面均朝上,你可能会想这货是不是有问题,其均匀的可能性是多少?这个硬币均匀就是参数,似然函数就是用来衡量这个模型的参数.似然也就是可能性的意思,它是关于统计参数的函数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯公式表示如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(B_i|A)=\\frac{P(B_i)P(A|B_i)}{\\sum_{i=1}^{n}P(B_i)P(A|B_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯是一种简单但极为强大的预测建模算法.之所以被称为朴素贝叶斯,是因为它假设每个输入变量是独立的.这是一个强硬的假设,实际情况并不一定,但是这项技术对于绝大部分的复杂问题仍然非常有效.该模型由两种类型的概率组成:\n",
    "- 每个类别的概率P(Cj);\n",
    "- 每个属性的条件概率P(Ai|Cj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类别概率与条件概率如何区分呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我有7个棋子,其中3个白棋,4个黑棋,因此棋子是白色的概率是3/7.这就是类别概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "又假设我把上面7个棋子放到两个盒子里,其中盒子A里有2个白棋,2个黑棋;盒子B里面有1个白棋,2个黑棋.那么在盒子中抓到白棋的概率就是1/2.这个就是条件概率,也就是在某个条件(比如在盒子A中)下的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在朴素贝叶斯中,我们要统计的是属性的条件概率,也就是假设取出来的是白棋,那么它属于盒子A的概率是2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯原理,贝叶斯分类器和朴素贝叶斯三者之间的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯原理是最大的概念,它解决了概率论中的逆向概率问题.在此基础上,人们设计出贝叶斯分类器,而朴素贝叶斯是贝叶斯分类器中的一种,也是最简单最常用的分类器.朴素贝叶斯之所以朴素是因为它假设属性性是相互独立的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/贝叶斯之原理概念间的关系.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn机器学习包中给我们提供了3个朴素贝叶斯分类算法,分别是高斯朴素贝叶斯(GaussianNB),多项式朴素贝叶斯(MultinomialNB)和伯努利朴素贝叶斯(BernoulliNB).这三种算法分别适用于不同的场景,我们根据特征变量选择其中之一.\n",
    "- 高斯朴素贝叶斯:特征变量是连续变量,符合高斯分布,比如人的身高体重,物体的长度\n",
    "- 多项式朴素贝叶斯:特征变量是离散变量,符合多项分布.在文档分类中特征变量体现在一个单词出现的次数,或者单词的TF-IDF值等\n",
    "- 伯努利朴素贝叶斯:特征变量是布尔变量,符合0/1分布,在文档分类中特征是单词是否出现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伯努利朴素贝叶斯是以文件为粒度,如果该单词在某文件中出现了即为1,否则为0.而多项式朴素贝叶斯是以单词为粒度,会计算在某个文件中的具体次数.而高斯朴素贝叶斯适合处理特征变量时连续变量且符合正态分布的情况,比如身高体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理.而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是TF-IDF值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF是一个统计方法,用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度.实际上它是两个词组的总称,即Term Frequency 和 Inverse Document Frequency ,分别代表了词频和逆向文档频率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词频TF** 计算了一个单词在文档中出现的次数,它认为一个单词的重要性和它在文档中出现的次数呈正比.  \n",
    "**逆向文档频率IDF** 是指一个单词在文档中的区分度.它认为一个单词出现在的文档数越少,就越能通过这个单词把该文档和其他文档区分开.IDF越大就代表该单词的区分度越大.  \n",
    "**TF-IDF实际上就是词频TF和逆向文档频率IDF的乘积**,原理就是当一个单词在一个文档中出现的次数越多,同时又很少出现在其他文档中.那么这个单词适合用于分类."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式表达如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "词频TF=\\frac{单词出现的次数}{该文档的总单词书}\n",
    "$$\n",
    "$$\n",
    "逆向文档频率IDF=log\\frac{文档总数}{该单词出现的文档数+1}\n",
    "$$\n",
    "$$\n",
    "TF-IDF=TF*IDF\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF分母加1的原因是有些单词可能不会存在文档中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个栗子.假设一个文件夹一共有10篇文档,其中一篇文档有1000个单词,this这个单词出现20次,bayes出现5次;this在素有文档中均出现过,bayes只在两篇文档中出现过.这两个单词的TF-IDF值计算过程如下:\n",
    "$$\n",
    "TF-IDF(this) =\\frac{20}{1000}*log\\frac{10}{10+1}=0.02*(-0.0414)=-8.28e-4\n",
    "$$\n",
    "$$\n",
    "TF-IDF(bayes)=\\frac{5}{1000}*log\\frac{10}{2+1}=0.005*0.5229=2.61e-3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然bayes的TF-IDF值要大于this.说明用bayes这个单词做区分相对要好."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer类的创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn中可以直接使用TfidfVectorizer类计算单词的TF-IDF向量的值,在这个类中,取sklearn计算的对数log时底数是e.具体创建方法如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中有两个构造参数,分别用于自定义停用词stop_words和规律规则token_pattern.这里注意前者是一个list类型,而过滤规则是正则表达式.  \n",
    "停用词就是在分类中没有用的次,这些词一般词频TF高同时IDF很低,起不到分类的作用.出于节省空间和计算时间的考虑,将其停用,告诉机器这些词不需要计算.  \n",
    "创建好TF-IDF向量类型,可以用fit_transform返回文本矩阵,表示每个单词在每个文档中的TF-IDF值.这之后我们可以的到更多的TF-IDF向量属性,比如词汇的对应关系(字典类型)和向量的IDF值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数表|作用\n",
    "---|---\n",
    "stop_words|自定义停用词表,为列表List类型\n",
    "token_pattern|过滤规则,正则表达式,若r\"(?u)\\b\\w+\\b\"\n",
    "fit_transform(X)|拟合模型,并返回文本矩阵\n",
    "vocabulary_|词汇表;字典型\n",
    "idf_|返回idf值\n",
    "stop_words_|返回停用词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何对文档进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档分类的两个重要阶段如下图所示:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/项目之文档分类关键流程.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于分词的数据准备,包括分词,单词权重计算,去掉停用词\n",
    "- 应用朴素贝叶斯分类进行分类,首先通过训练集得到朴素贝叶斯分类器,然后将分类器应用于测试集,并与实际结果做对比,最终得到测试集的分类准确率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对文档进行分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备阶段最重要的就是分词.这里注意英文文档和中文文档分词工具是不同的.  \n",
    "在英文文档中,常用nltk包.nltk包包含了英文的停用词stop words,分词和标注方法  \n",
    "在中文文档中,常用jieba包.jieba包中包含中文的停用词stop words 和分词方法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# word_list=nltk.word_tokenize(text) # 英文分词\n",
    "# nltk.pos_tag(word_list) # 标注单词的词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jieba\n",
    "# word_list=jieba.cut(text) # 中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载停用词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从网上可以找到中文常用的停用词,保存在stop_words.txt中,然后利用Python的文件读取函数读取文件,并保存在stop_words数组中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# stop_words=[line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算单词的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建TfidfVectorizer类,然后使用fit_transform方法进行拟合,得到TF-IDF特征空间features,可以理解为选出来的的分词就是特征.通过计算这些特征在文档上的特征向量,得到特征空间features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "# features = tf.fit_transform(train_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df参数用来描述单词在文档中的最高出现率.假设max_df=0.5,代表一个单词在50%的文档中都出现过,那么它只携带了非常少的信息,因此就不作为分词统计.(一般很少设置min_df,因为min_df通常都会很小)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将特征训练集的特征空间train_features以及训练集对应的(分类)标签train_labels传递给贝叶斯分类器clf,它会自动生成一个符合特征空间和对应分类的分类器."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里采用多项式贝叶斯分类器,其中alpha为平滑参数.   \n",
    "当alpha=1时,使用Laplace平滑,其采用加1的方式来统计没有出现过的单词的概率,这样在训练样本很大的时候加1得到的概率变化可以忽略不计,同时也避免了零概率的问题.  \n",
    "当)<alpha<1时,使用Lidstone平滑,这样当alpha越小,迭代次数越多,精度越高.这里我们设置alpha为0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 多想贝叶斯分类器\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf=MultinomialNB(alpha=0.001).fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用生成的分类器做预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先需要得到测试集的特征矩阵.用训练集的分词创建一个TfidfVectorizer类,使用相同的stop_words和max_df,然后用这个类对测试集的内容进行fit_transform拟合,得到测试集的特征矩阵test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tf = TfidfVectorizer(\n",
    "#     stop_word=stop_words, max_df=0.5, vocabulary=train_vocabulary)\n",
    "# test_features = test_tf.fit_transform(test_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后用训练的好的分类器对新数据做预测.使用predict函数传入测试集的特征矩阵test_features,得到分类结果predicted_labels.  \n",
    "注意predict函数做的工作就是求解所有后验概率并找出最大的那个."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上就是对分类模型做评估.调用sklearn中的metrics包,该包提供了accuracy_score函数可以实际结果和预测结果对比,从而给出模型的准确率."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# print(metrics.accuracy_score(test_labels,predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先,在文档分类项目中,针对文档的特点给出了基于分词的准备流程.可以根据文档选择不同的包,一般来说英文文档调用NTLK包,中文文档调用jieba包,实现对文档提取分词.这些文辞就相当于贝叶斯分类中最重要的特征属性.基于这些分词得到分词的权重,即特征矩阵.  \n",
    "然后,通过特征矩阵与分类结果,我们就可以创建朴素贝叶斯分类器,然后用分类器进行预测.  \n",
    "最后预测结果与实际结果做对比来评估模型."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis]",
   "language": "python",
   "name": "conda-env-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
